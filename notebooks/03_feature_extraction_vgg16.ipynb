{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import permute,avg_pool1d\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from analysis import label_tools as lt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre processed files\n",
    "imgs_path = Path('/home/richard/data/Schiefer/combine/preprocessed_224_clipped_pred_polygon_1126/')\n",
    "assert imgs_path.is_dir()\n",
    "files = sorted(imgs_path.glob('*.png'))\n",
    "\n",
    "randomizer = np.random.RandomState(seed=99834)\n",
    "randomizer.shuffle(files)\n",
    "\n",
    "assert len(files) == 1126 # all files are found\n",
    "print(\"First 10 files are: {}\".format([x.name for x in files[:10]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def alter_image(img_name):\n",
    "    image = cv2.imread(str(img_name))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = normalize(to_tensor(image)).unsqueeze(0)\n",
    "    image_tensor = image_tensor.reshape(1,3,224,224)\n",
    "    return image_tensor\n",
    "\n",
    "def load_images_as_tensors(paths):\n",
    "    images = [alter_image(image) for image in paths]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors = load_images_as_tensors(files)\n",
    "assert len(image_tensors) == 1126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting labels from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(files): return [filename.stem.split('_')[4] for filename in files]\n",
    "labels = extract_labels(files)\n",
    "print('first 10 labels: {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change labels from number to species (of collab)\n",
    "update = {\n",
    "    '4':'Fagus_sylvatica',\n",
    "    '5':'Fraxinus_excelsior',\n",
    "    '6':'Quercus_spec',\n",
    "    '8':'deadwood',\n",
    "    '10':'Abies_alba',\n",
    "    '11':'Larix_decidua',\n",
    "    '12':'Picea_abies',\n",
    "    '13':'Pinus_sylvestris',\n",
    "    '14':'Pseudotsuga_menziesii'\n",
    "}\n",
    "\n",
    "updated_labels = (pd.Series(labels)).map(update)\n",
    "species_labels = list(updated_labels)\n",
    "labels = species_labels\n",
    "print('first 10 labels: {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding\n",
    "\n",
    "Standardize encodings of labels to make analysis easier afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = lt.CustomLabelEncoder()\n",
    "le.fit(labels, sorter=lambda x: x.upper())\n",
    "\n",
    "labels_int = le.transform(labels[:10])\n",
    "labels_str = le.inverse_transform(labels_int)\n",
    "\n",
    "label_dir = Path('/home/richard/data/Schiefer/combine/')\n",
    "filename = Path('VGG16_polygon_pred_label_encodings_224_' + str(imgs_path).split('_')[5] + '.pickle')\n",
    "with open(label_dir / filename, 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('label encodings: {}'.format(le.mapper))\n",
    "print('first 10 integer labels: {}'.format(labels_int))\n",
    "print('first 10 string labels: {}'.format(labels_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for avgpool layer features 512\n",
    "# model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "\t\t# Extract VGG-16 Feature Layers\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "\t\t# Extract VGG-16 Average Pooling Layer\n",
    "    self.pooling = model.avgpool\n",
    "\t\t# Convert the image into one-dimensional vector\n",
    "    self.flatten = nn.Flatten()\n",
    "\t\t# Extract the first part of fully-connected layer from VGG16\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    out = self.fc(out) \n",
    "    return out \n",
    "\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "new_model = FeatureExtractor(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get FC1 features of the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(tensor):\n",
    "    with torch.no_grad():\n",
    "        feature = new_model(tensor)\n",
    "    return feature.cpu().detach().numpy()\n",
    "   \n",
    "\n",
    "def concat_tensors(features):\n",
    "    fc = torch.cat(features)\n",
    "    fc = fc.cpu().detach().numpy()\n",
    "    return fc\n",
    "\n",
    "def get_features(tensors):\n",
    "    with torch.no_grad():\n",
    "        features = [model(tensor) for tensor in tqdm(tensors)]\n",
    "    features = concat_tensors(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(get_feature(image_tensors[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1 = get_features(image_tensors)\n",
    "# print(fc1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get avgpool features (flattened 7 x 7 x 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pooled_features(tensors):\n",
    "#     features = [model(tensor) for tensor in tqdm(tensors)]\n",
    "#     print(\"done first\")\n",
    "#     features = [torch.flatten(tensor, 1) for tensor in tensors]\n",
    "#     features = concat_tensors(features)\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1 = get_pooled_features(image_tensors)\n",
    "# print(fc1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "results = {'filename': files,\n",
    "           'features': fc1,\n",
    "           'labels': labels,\n",
    "           'layer_name': 'fc1'}\n",
    "\n",
    "feature_dir = Path('/home/richard/data/Schiefer/combine/')\n",
    "feature_filename = Path('VGG16_polygon_pred_224_' + str(imgs_path).split('_')[5] + '.pickle')\n",
    "Path(feature_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(feature_dir / feature_filename, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "579d37c0e11f829fd27710afca693474f5bbc510c142a7662cee2b0a86b87b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
