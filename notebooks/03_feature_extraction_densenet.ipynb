{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import permute\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO module docstring, examples for functions\n",
    "\n",
    "class CustomLabelEncoder:\n",
    "    \"\"\"\n",
    "    Creates a mapping between string labels and integer class clabels for working with categorical data.\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    mapper:None dict\n",
    "        None if mapper is not supplied or model is not fit.\n",
    "        keys are unique string labels, values are integer class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, mapper=None):\n",
    "        \"\"\"\n",
    "        Initializes class instance.\n",
    "        \n",
    "        If the mapper dictionary is supplied here, then the model can be used without calling .fit().\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        mapper (optional): dict or None\n",
    "            if mapper is None encoder will need to be fit to data before it can be used.\n",
    "            If it is a dictionary mapping string labels to integer class labels, then this will be stored\n",
    "            and the model can be used to transform data.\n",
    "        \"\"\"\n",
    "        self.mapper = mapper\n",
    "    \n",
    "    def fit(self, str_labels, sorter=None):\n",
    "        \"\"\"\n",
    "        Fits string labels to intiger indices with optional sorting.\n",
    "        \n",
    "        np.unique() is used to extract the unique values form labels. If \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        str_labels: list-like\n",
    "            list or array containing string labels\n",
    "        \n",
    "        sorter (optional): None or function\n",
    "            key for calling sorted() on data to determine ordering of the numeric indices for each label.\n",
    "            \n",
    "        Attributes\n",
    "        -----------\n",
    "        mapper: dict\n",
    "            dictionary mapping string labels to the sorted integer indices is stored after fitting.\n",
    "        \n",
    "        \"\"\"\n",
    "        sorted_unique = sorted(np.unique(str_labels), key=sorter)\n",
    "        mapper = {label: i for i, label in enumerate(sorted_unique)}\n",
    "        self.mapper = mapper    \n",
    "\n",
    "    def transform(self, str_labels):\n",
    "        \"\"\"\n",
    "        Maps string labels to integer labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        str_labels: list-like\n",
    "            list of string labels whose elements are in self.mapper\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        int_labels: array\n",
    "            array of integer labels  corresponding to the string labels\n",
    "        \"\"\"\n",
    "        assert self.mapper is not None, 'Encoder not fit yet!'\n",
    "        \n",
    "        int_labels = np.asarray([self.mapper[x] for x in str_labels], np.int)\n",
    "        \n",
    "        return int_labels\n",
    "        \n",
    "    def inverse_transform(self, int_labels):\n",
    "        \"\"\"\n",
    "        Maps integer labels to original string labels.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        int_labels: list-like\n",
    "            list or array of integer class indices\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        str_labels: array(str)\n",
    "            array of string labels corresponding to intiger indices\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.mapper is not None, 'Encoder not fit yet!'\n",
    "        \n",
    "        reverse_mapper = {y:x for x,y in self.mapper.items()}\n",
    "        \n",
    "        str_labels = np.asarray([reverse_mapper[x] for x in int_labels])\n",
    "        \n",
    "        return str_labels\n",
    "    \n",
    "    @property\n",
    "    def labels_ordered(self):\n",
    "        \"\"\"\n",
    "        Returns an array containing the string labels in order of which they are stored.\n",
    "        \n",
    "        For example, if the label_encoder has the following encoding: {'a':1,'c':3,'b':2},\n",
    "        then this will return array(['a','b','c'])\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @labels_ordered.getter\n",
    "    def labels_ordered(self):\n",
    "        return self.inverse_transform(range(len(self.mapper)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre processed files\n",
    "imgs_path = Path('/home/richard/data/Schiefer/preprocessed_None_clahe_clipped_pred_polygon_204/')\n",
    "assert imgs_path.is_dir()\n",
    "files = sorted(imgs_path.glob('*.png'))\n",
    "\n",
    "randomizer = np.random.RandomState(seed=99837)\n",
    "randomizer.shuffle(files)\n",
    "\n",
    "assert len(files) == int(str(imgs_path).split('_')[6]) # all files are found\n",
    "print(\"First 10 files are: {}\".format([x.name for x in files[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize((224,224))\n",
    "\n",
    "def alter_image(img_name):\n",
    "    image = Image.open(img_name)\n",
    "    image = resize(image)\n",
    "    image_tensor = normalize(to_tensor(image)).unsqueeze(0)\n",
    "    image_tensor = image_tensor.reshape(1,3,224,224)\n",
    "    return image_tensor\n",
    "\n",
    "def load_images_as_tensors(paths):\n",
    "    images = [alter_image(image) for image in paths]\n",
    "    return images\n",
    "\n",
    "image_tensors = load_images_as_tensors(files)\n",
    "assert len(image_tensors) == int(str(imgs_path).split('_')[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting labels from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(files): return [filename.stem.split('_')[5] for filename in files]\n",
    "labels = extract_labels(files)\n",
    "print('first 10 labels: {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change labels from number to species\n",
    "update = {\n",
    "    '0':'other',\n",
    "    '4':'Fagus_sylvatica',\n",
    "    '8':'deadwood',\n",
    "    '10':'Abies_alba',\n",
    "    '12':'Picea_abies'\n",
    "}\n",
    "\n",
    "updated_labels = (pd.Series(labels)).map(update)\n",
    "species_labels = list(updated_labels)\n",
    "labels = species_labels\n",
    "print('first 10 labels: {}'.format(labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding\n",
    "\n",
    "Standardize encodings of labels to make analysis easier afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = CustomLabelEncoder()\n",
    "le.fit(labels, sorter=lambda x: x.upper())\n",
    "\n",
    "labels_int = le.transform(labels[:10])\n",
    "labels_str = le.inverse_transform(labels_int)\n",
    "\n",
    "with open(Path('/home/richard/data/Schiefer/features/label_encoder_non_normalized.pickle'), 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('label encodings: {}'.format(le.mapper))\n",
    "print('first 10 integer labels: {}'.format(labels_int))\n",
    "print('first 10 string labels: {}'.format(labels_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "Load the ResNet152 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1)\n",
    "layer = model._modules.get('avgpool')\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(image_tensor):\n",
    "    # a dict to store the activations\n",
    "    activation = {}\n",
    "    def getActivation(name):\n",
    "        # the hook signature\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # register forward hooks on the layers of choice\n",
    "    h1 = model.register_forward_hook(getActivation('avg_last'))\n",
    "\n",
    "    # forward pass -- getting the outputs\n",
    "    out = model(image_tensor)\n",
    "\n",
    "    # detach the hooks\n",
    "    h1.remove()\n",
    "    \n",
    "    feature = torch.squeeze(activation['avg_last'])\n",
    "    feature = torch.unsqueeze(feature,dim=0)\n",
    "    return feature\n",
    "\n",
    "# check\n",
    "feature = get_feature_vector(image_tensors[0])\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all features of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensors(features):\n",
    "    fc = torch.cat(features)\n",
    "    fc = fc.cpu().detach().numpy()\n",
    "    return fc\n",
    "\n",
    "def get_features(image_tensors):\n",
    "    features = [get_feature_vector(tensor) for tensor in image_tensors]\n",
    "    features = concat_tensors(features)\n",
    "    return features\n",
    "\n",
    "fc = get_features(image_tensors)\n",
    "print(fc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'filename': files,\n",
    "           'features': fc,\n",
    "           'labels': labels,\n",
    "           'layer_name': 'fc'}\n",
    "\n",
    "feature_dir = imgs_path.parent / 'features'\n",
    "Path(feature_dir).mkdir(parents=True, exist_ok=True)\n",
    "with open(feature_dir / 'Inception_v3_feature_std_224_normalized.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "579d37c0e11f829fd27710afca693474f5bbc510c142a7662cee2b0a86b87b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
