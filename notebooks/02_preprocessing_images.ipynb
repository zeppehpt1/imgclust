{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Feature \n",
    "\n",
    "- Before proceed, check if number of polygons matches the number of generated files (polygon-boxes, squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.exposure as skie\n",
    "import skimage\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import tqdm\n",
    "from cv2 import normalize\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image,ImageOps\n",
    "import mahotas\n",
    "from pylab import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/richard/data/Schiefer/tests/CFB184_clipped_raster_files_pred/CFB184_ortho_rgb_mask_0108_10.png' # tanne\n",
    "#img_path = '/home/richard/data/Schiefer/tests/CFB184_clipped_raster_files_pred/CFB184_ortho_rgb_mask_0013_12.png' # kiefer\n",
    "#img_path = '/home/richard/data/Schiefer/tests/CFB184_clipped_raster_files_pred/CFB184_ortho_rgb_mask_0109_8.png' # deadwood\n",
    "img = cv2.imread(img_path)\n",
    "#img = skimage.color.rgb2gray(img) # preserves luminance of img\n",
    "print(type(img))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mahotas RGB image stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mahotas.imread(img_path)\n",
    "new_img = mahotas.stretch_rgb(img)\n",
    "imshow(new_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_c = img\n",
    "\n",
    "dst = cv2.fastNlMeansDenoisingColored(img_c,None,10,10,7,21)\n",
    "dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "dst = skie.equalize_adapthist(dst)\n",
    "\n",
    "img_c = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img)\n",
    "plt.subplot(122),plt.imshow(dst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_c = img\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4),\n",
    "                               sharex=True, sharey=True)\n",
    "\n",
    "img0 = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "img0 = ax0.imshow(img0, cmap=plt.cm.gray)\n",
    "ax0.set_title(\"Original img\")\n",
    "ax0.axis(\"off\")\n",
    "\n",
    "img1 = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "img1 = ax1.imshow(skie.equalize_adapthist(img1))\n",
    "ax1.set_title(\"CLAHE\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "img2 = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "img2 = skie.equalize_adapthist(img2)\n",
    "img2 = cv2.normalize(img2, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "img2 = img2.astype(np.uint8)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n",
    "img2 = cv2.fastNlMeansDenoisingColored(img2, None,10,10,7,21)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "img2 = ax2.imshow(img2, cmap=plt.cm.gray)\n",
    "ax2.set_title(\"CLAHE then denoising\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_c = img\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4),\n",
    "                               sharex=True, sharey=True)\n",
    "\n",
    "img0 = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "img0 = ax0.imshow(img0, cmap=plt.cm.gray)\n",
    "ax0.set_title(\"Original img\")\n",
    "ax0.axis(\"off\")\n",
    "\n",
    "img1 = cv2.cvtColor(img_c, cv2.COLOR_BGR2RGB)\n",
    "img1 = ax1.imshow(skie.equalize_adapthist(img1))\n",
    "ax1.set_title(\"CLAHE\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "img2 = cv2.fastNlMeansDenoisingColored(img_c, None,10,10,7,21)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "img2 = skie.equalize_adapthist(img2)\n",
    "img2 = cv2.normalize(img2, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "img2 = img2.astype(np.uint8)\n",
    "img2 = ax2.imshow(img2, cmap=plt.cm.gray)\n",
    "ax2.set_title(\"Denoising then CLAHE\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Variation Filter (Blur)\n",
    "\n",
    "- tackles signals with excessive spurious detail have high variation\n",
    "- reducing the total variation of the signal, removes unwanted detail while preserving important details such as edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 4),\n",
    "                               sharex=True, sharey=True)\n",
    "\n",
    "img0 = ax0.imshow(img, cmap=plt.cm.gray)\n",
    "ax0.set_title(\"CLAHE Image\")\n",
    "ax0.axis(\"off\")\n",
    "fig.colorbar(img0, ax=ax0)\n",
    "\n",
    "img1 = ax1.imshow(denoise_tv_chambolle(img, weight=0.1, channel_axis=-1))\n",
    "ax1.set_title(\"TVF\")\n",
    "ax1.axis(\"off\")\n",
    "fig.colorbar(img1, ax=ax1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramm normalization\n",
    "\n",
    "- with CLAHE\n",
    "  - Two benefits: normalize brightness, reduce differences between darker and light images\n",
    "  - enhance contrast of image (stronger responses in conv layers)\n",
    "\n",
    "Kinda only useful for square images otherwise black pixels are appearing too often, but still brightness is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img): #displays image next to a histogram\n",
    "    # Display the image.\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,\n",
    "                                   figsize=(12, 3))\n",
    "\n",
    "    ax1.imshow(img, cmap=plt.cm.gray)\n",
    "    ax1.set_axis_off()\n",
    "\n",
    "    # Display the histogram.\n",
    "    ax2.hist(img.ravel(), lw=0, bins=256)\n",
    "    ax2.set_xlim(0, img.max())\n",
    "    ax2.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast Limited Adaptive Histogram Equalization (CLAHE) applied\n",
    "show(skie.equalize_adapthist(img))\n",
    "# under the hood works rescale_intensity, equalize_adapthist,\n",
    "\n",
    "clahe_image = skie.equalize_adapthist(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 4),\n",
    "                               sharex=True, sharey=True)\n",
    "\n",
    "img0 = ax0.imshow(clahe_image, cmap=plt.cm.gray)\n",
    "ax0.set_title(\"CLAHE image\")\n",
    "ax0.axis(\"off\")\n",
    "\n",
    "img1 = ax1.imshow(denoise_tv_chambolle(clahe_image, weight=0.1, channel_axis=-1))\n",
    "ax1.set_title(\"TVF\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = skie.equalize_adapthist(img)\n",
    "print(type(new))\n",
    "PIL_image = Image.fromarray((new * 255).astype(np.uint8))\n",
    "plt.imshow(PIL_image)\n",
    "print(PIL_image.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrast Stretching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize images\n",
    "\n",
    "VGG16 and ResNet default size = 224. Use this size for square crowns. So they can be directly fed into the CNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size - img.size[0]\n",
    "    delta_height = desired_size - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "def resize_with_padding(img, expected_size):\n",
    "    img.thumbnail((expected_size[0], expected_size[1]))\n",
    "    # print(img.size)\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "def pil_resize(img, expected_size):\n",
    "    image = img\n",
    "    new_image = image.resize((expected_size,expected_size))\n",
    "    return new_image\n",
    "\n",
    "def get_png_file_names(file_dir):\n",
    "    files = glob.glob(file_dir + '/*.png')\n",
    "    print(\"num files\",len(files))\n",
    "    return files\n",
    "\n",
    "def normalize_img(img_arr):\n",
    "    normalized_img = cv2.normalize(img_arr, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    normalized_img = normalized_img.astype(np.uint8)\n",
    "    return normalized_img\n",
    "\n",
    "def highest_pixel_count(img_array):\n",
    "    pixel, n_of_pixels = np.unique(img_array, return_counts=True)\n",
    "    highest_pixel_value = pixel[np.argsort(-n_of_pixels)]\n",
    "    index = 0\n",
    "    forbidden_values = {0,1,2}\n",
    "    while True:\n",
    "        if highest_pixel_value[index] not in forbidden_values:\n",
    "            return highest_pixel_value[index]\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\"_summary_\n",
    "    do it on orthoreference images from schiefer and count the pixels that lie inside the ground truth of schiefer\n",
    "    \"\"\"\n",
    "\n",
    "def check_alpha_channel(img_arr):\n",
    "    h,w,c = img_arr.shape\n",
    "    return True if c ==4 else False\n",
    "\n",
    "def preprocess_images(images_path, expected_size, square):\n",
    "    file_list = get_png_file_names(images_path)\n",
    "    shape = 'polygon'\n",
    "    if square:\n",
    "        shape = 'square'\n",
    "    out_dir = Path(images_path).parent / Path('preprocessed_' + str(expected_size) + '_clipped_pred_' + shape + '_' + str(len(file_list)))\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for img in (pbar := tqdm.tqdm(file_list)):\n",
    "        pbar.set_description(f\"Processing {img}\")\n",
    "        file_name = Path(img).stem\n",
    "        # img = cv2.imread(img) loads image in BGR order!\n",
    "        img = plt.imread(img) # loads image in RGB order\n",
    "        # remove alpha if image has alpha (TODO: handle automatically)\n",
    "        processed_img = img[:,:,:3]\n",
    "        # denoise image\n",
    "        # processed_img = cv2.fastNlMeansDenoisingColored(processed_img,None,10,10,7,21) # increases processing time\n",
    "        # normalize brightness CLAHE\n",
    "        processed_img = skie.equalize_adapthist(img)\n",
    "        #processed_img = denoise_tv_chambolle(img, weight=0.1, channel_axis=-1)\n",
    "        # normalize pixel values to range 0-255\n",
    "        processed_img = normalize_img(processed_img)\n",
    "        # calc pixel count\n",
    "        #pixel_count = highest_pixel_count(processed_img)\n",
    "        # array to img\n",
    "        #PIL_image = Image.fromarray((processed_img * 255).astype(np.uint8)) # default method not normalized pixel\n",
    "        PIL_image = Image.fromarray((processed_img).astype(np.uint8)) # if normalized\n",
    "        # resize\n",
    "        PIL_image = resize_with_padding(PIL_image,(expected_size,expected_size))\n",
    "        #PIL_image = PIL_image.resize((expected_size, expected_size)) # use for squares (strecthed) TODO: implement flag to chose from both\n",
    "        PIL_image.save(str(out_dir) + '/' + str(file_name) + '_preprocessed' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple\n",
    "polygon_crops_path = '/home/richard/data/Schiefer/combine/pred_polygon_clipped_raster_files'\n",
    "assert Path(polygon_crops_path).is_dir()\n",
    "\n",
    "print(Path(polygon_crops_path).parent / 'preprocessed')\n",
    "\n",
    "preprocess_images(polygon_crops_path, 512, square=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_crops_path = '/home/richard/data/Schiefer/feature_extraction/clips/CFB184_clipped_raster_files_pred_square/'\n",
    "polygon_crops_path = '/home/richard/data/Schiefer/feature_extraction/clips/CFB184_clipped_raster_files_pred_polygon/'\n",
    "\n",
    "square_gt_crops_path = '/home/richard/data/Schiefer/feature_extraction/clips/CFB184_clipped_raster_files_gt_square/'\n",
    "polygon__gt_crops_path = '/home/richard/data/Schiefer/feature_extraction/clips/CFB184_clipped_raster_files_gt_polygon/'\n",
    "\n",
    "assert Path(polygon_crops_path).is_dir()\n",
    "assert Path(square_crops_path).is_dir()\n",
    "\n",
    "print(Path(square_crops_path).parent / 'preprocessed')\n",
    "\n",
    "preprocess_images(polygon__gt_crops_path,224, square=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = '/home/richard/data/Schiefer/preprocessedwithpixel_224/'\n",
    "files = glob.glob(imgs_dir + '*.png')\n",
    "num = []\n",
    "for f in files:\n",
    "    f = Path(f).stem\n",
    "    f = f.split('_')[5]\n",
    "    num.append(f)\n",
    "\n",
    "unique, counts = np.unique(num,return_counts=True)\n",
    "counts = sorted(counts,reverse=True)\n",
    "print(\"unique values\",len(unique))\n",
    "for value, number in zip(unique, counts):\n",
    "    print(value,number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = '/home/richard/data/Schiefer/tests/CFB184_clipped_raster_files_squares/'\n",
    "files = glob.glob(imgs_dir + '*.png')\n",
    "print(len(files))\n",
    "num = []\n",
    "for f in files:\n",
    "    f = Path(f).stem\n",
    "    f = f.split('_')[4]\n",
    "    num.append(f)\n",
    "\n",
    "unique, counts = np.unique(num,return_counts=True)\n",
    "counts = sorted(counts,reverse=True)\n",
    "print(\"unique values\",len(unique))\n",
    "for value, number in zip(unique, counts):\n",
    "    print(value,number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one image\n",
    "img_path = '/home/richard/data/Schiefer/preprocessed_224/CFB184_ortho_rgb_0002_preprocessed.png'\n",
    "assert Path(img_path).exists()\n",
    "\n",
    "img_arr = plt.imread(img_path)\n",
    "print(type(img_arr))\n",
    "pixel, n_of_pixels = np.unique(img_arr, return_counts=True)\n",
    "highest_pixel_value = pixel[np.argsort(-n_of_pixels)]\n",
    "print(type(highest_pixel_value[2]))\n",
    "\n",
    "\n",
    "new_img = normalize(img_arr, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "new_img = new_img.astype(np.uint8)\n",
    "print(type(new_img))\n",
    "plt.imshow(new_img)\n",
    "print(type(new_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img_arr):\n",
    "    normalized_img = cv2.normalize(img_arr, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    normalized_img = normalized_img.astype(np.uint8)\n",
    "    return normalized_img\n",
    "\n",
    "def highest_pixel_count(img_array):\n",
    "    pixel, n_of_pixels = np.unique(img_array, return_counts=True)\n",
    "    highest_pixel_value = pixel[np.argsort(-n_of_pixels)]\n",
    "    index = 0\n",
    "    forbidden_values = {0,1}\n",
    "    while True:\n",
    "        if highest_pixel_value[index] not in forbidden_values:\n",
    "            return highest_pixel_value[index]\n",
    "        index += 1\n",
    "\n",
    "\n",
    "new = normalize_img(img_arr)\n",
    "count = highest_pixel_count(new)\n",
    "\n",
    "\n",
    "# vis\n",
    "PIL_image = Image.fromarray(new)\n",
    "plt.imshow(PIL_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve selecting, skip if highest pixel count == 0 or 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "579d37c0e11f829fd27710afca693474f5bbc510c142a7662cee2b0a86b87b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
