{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec, cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import skimage.io\n",
    "import yellowbrick\n",
    "from skimage.feature import hog\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS, SpectralClustering, AgglomerativeClustering, FeatureAgglomeration, Birch, AffinityPropagation, MeanShift, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA,KernelPCA, SparsePCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, classification_report, silhouette_samples, silhouette_score, calinski_harabasz_score, davies_bouldin_score, matthews_corrcoef, cohen_kappa_score, log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "from fcmeans import FCM\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import shapiro, kstest, normaltest, anderson\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "class CustomLabelEncoder:\n",
    "    \"\"\"\n",
    "    Creates a mapping between string labels and integer class clabels for working with categorical data.\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    mapper:None dict\n",
    "        None if mapper is not supplied or model is not fit.\n",
    "        keys are unique string labels, values are integer class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, mapper=None):\n",
    "        \"\"\"\n",
    "        Initializes class instance.\n",
    "        \n",
    "        If the mapper dictionary is supplied here, then the model can be used without calling .fit().\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        mapper (optional): dict or None\n",
    "            if mapper is None encoder will need to be fit to data before it can be used.\n",
    "            If it is a dictionary mapping string labels to integer class labels, then this will be stored\n",
    "            and the model can be used to transform data.\n",
    "        \"\"\"\n",
    "        self.mapper = mapper\n",
    "    \n",
    "    def fit(self, str_labels, sorter=None):\n",
    "        \"\"\"\n",
    "        Fits string labels to intiger indices with optional sorting.\n",
    "        \n",
    "        np.unique() is used to extract the unique values form labels. If \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        str_labels: list-like\n",
    "            list or array containing string labels\n",
    "        \n",
    "        sorter (optional): None or function\n",
    "            key for calling sorted() on data to determine ordering of the numeric indices for each label.\n",
    "            \n",
    "        Attributes\n",
    "        -----------\n",
    "        mapper: dict\n",
    "            dictionary mapping string labels to the sorted integer indices is stored after fitting.\n",
    "        \n",
    "        \"\"\"\n",
    "        sorted_unique = sorted(np.unique(str_labels), key=sorter)\n",
    "        mapper = {label: i for i, label in enumerate(sorted_unique)}\n",
    "        self.mapper = mapper    \n",
    "\n",
    "    def transform(self, str_labels):\n",
    "        \"\"\"\n",
    "        Maps string labels to integer labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        str_labels: list-like\n",
    "            list of string labels whose elements are in self.mapper\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        int_labels: array\n",
    "            array of integer labels  corresponding to the string labels\n",
    "        \"\"\"\n",
    "        assert self.mapper is not None, 'Encoder not fit yet!'\n",
    "        \n",
    "        int_labels = np.asarray([self.mapper[x] for x in str_labels], np.int)\n",
    "        \n",
    "        return int_labels\n",
    "        \n",
    "    def inverse_transform(self, int_labels):\n",
    "        \"\"\"\n",
    "        Maps integer labels to original string labels.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        int_labels: list-like\n",
    "            list or array of integer class indices\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        str_labels: array(str)\n",
    "            array of string labels corresponding to intiger indices\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.mapper is not None, 'Encoder not fit yet!'\n",
    "        \n",
    "        reverse_mapper = {y:x for x,y in self.mapper.items()}\n",
    "        \n",
    "        str_labels = np.asarray([reverse_mapper[x] for x in int_labels])\n",
    "        \n",
    "        return str_labels\n",
    "    \n",
    "    @property\n",
    "    def labels_ordered(self):\n",
    "        \"\"\"\n",
    "        Returns an array containing the string labels in order of which they are stored.\n",
    "        \n",
    "        For example, if the label_encoder has the following encoding: {'a':1,'c':3,'b':2},\n",
    "        then this will return array(['a','b','c'])\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @labels_ordered.getter\n",
    "    def labels_ordered(self):\n",
    "        return self.inverse_transform(range(len(self.mapper)))\n",
    "    \n",
    "def label_matcher(y_cluster, labels, return_mapper=False):\n",
    "    \"\"\"\n",
    "    maps cluster centers to true labels based on the most common filename for each cluster. \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_cluster: ndarray\n",
    "        n-element array of labels obtained from clusters\n",
    "        \n",
    "    labels: ndarray\n",
    "        n-element array of ground truth labels for which y_cluster will be mapped to\n",
    "        \n",
    "    return_mapper:bool\n",
    "        if True, dictionary mapping values in y_cluster to values in labels will be returned\n",
    "    Returns\n",
    "    -----------\n",
    "    y_pred: ndarray\n",
    "        n-element array of values in y_cluster mapped to labels\n",
    "    \n",
    "    mapper (optional): dict\n",
    "        dictonary whose keys are elements of y_cluster and values are the corresponding\n",
    "        elements of labels.\n",
    "    \"\"\"\n",
    "        \n",
    "    y_cluster = np.asarray(y_cluster)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    y_cluster_unique = np.unique(y_cluster)\n",
    "\n",
    "    \n",
    "    mapper = {}  # keys will be cluster ID's, values will be corresponding label\n",
    "    \n",
    "    for x in y_cluster_unique:\n",
    "        unique, counts = np.unique(labels[y_cluster==x], return_counts=True)  # get frequency of each gt label in cluster x\n",
    "        mapper[x] = unique[counts.argmax()]  # set mapper[x] to the most frequent label in the cluster\n",
    "\n",
    "    y_pred = np.asarray([mapper[x] for x in y_cluster])  # map cluster id's to labels\n",
    "\n",
    "    if return_mapper:\n",
    "        return y_pred, mapper\n",
    "    else:\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/VGG16_fc1_feature_std.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/VGG16_fc1_feature_std_224.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std_224.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std_224_normalized.pickle') # OG!\n",
    "# fc1_path = Path('/home/richard/data/Schiefer/tests/features/ResNet152_polygon_gt.pickle') # gt bad\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/tests/features/ResNet152_squares_pred.pickle') # og corrected squares\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std_squares_224_normalized.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/tests/features/ResNet152_feature_std_squares_224_normalized.pickle') # SQUARES 224, good results\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std_448_normalized.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/ResNet152_feature_std_224_non_normalized.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/Inception_v3_feature_std_224_normalized.pickle') # also good\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/MobileNet_v3_feature_std_224_normalized.pickle')\n",
    "\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/tests/features/ResNet152_polygon_pred.pickle') # og corrected\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/features/concat_polygon_pred.pickle') # og corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc1_path = Path('/home/richard/data/Schiefer/feature_extraction/features/ResNet152_gt_polygon_cfb184_296.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/feature_extraction/features/ResNet152_gt_square_cfb184_285.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/feature_extraction/features/ResNet152_pred_polygon_cfb184_224.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/feature_extraction/features/ResNet152_pred_square_cfb184_205.pickle')\n",
    "\n",
    "fc1_path = Path('/home/richard/data/Schiefer/combine/ResNet152_polygon_pred_1126.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/combine/ResNet152_square_pred_1140.pickle')\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/combine/ResNet152_polygon_pred_512_1126.pickle')\n",
    "\n",
    "#fc1_path = Path('/home/richard/data/Schiefer/combine/VGG16_polygon_pred_224_1126.pickle')\n",
    "\n",
    "######\n",
    "fc1_path = Path('/home/richard/data/Schiefer/encodings/resnet_clahe-denoising_polygon_pred.pickle')\n",
    "assert fc1_path.is_file()\n",
    "# TODO: change name scheme of file in step 3\n",
    "\n",
    "#le_path = Path('/home/richard/data/Schiefer/feature_extraction/labels/ResNet152_gt_polygon_label_encodings_cfb184_296.pickle')\n",
    "#le_path = Path('/home/richard/data/Schiefer/feature_extraction/labels/ResNet152_gt_square_label_encodings_cfb184_285.pickle')\n",
    "#le_path = Path('/home/richard/data/Schiefer/feature_extraction/labels/ResNet152_pred_polygon_label_encodings_cfb184_224.pickle')\n",
    "#le_path = Path('/home/richard/data/Schiefer/feature_extraction/labels/ResNet152_pred_square_label_encodings_cfb184_205.pickle')\n",
    "\n",
    "le_path = Path('/home/richard/data/Schiefer/combine/ResNet152_polygon_pred_label_encodings_1126.pickle')\n",
    "#le_path = Path('/home/richard/data/Schiefer/combine/ResNet152_square_pred_label_encodings_1140.pickle')\n",
    "#le_path = Path('/home/richard/data/Schiefer/combine/ResNet152_polygon_pred_label_encodings_512_1126.pickle')\n",
    "\n",
    "#le_path = Path('/home/richard/data/Schiefer/combine/VGG16_polygon_pred_label_encodings_224_1126.pickle')\n",
    "\n",
    "######\n",
    "le_path = Path('/home/richard/data/Schiefer/label_encodings/resnet_clahe-denoising_label_encodings.pickle')\n",
    "assert le_path.is_file()\n",
    "\n",
    "# load the data and label encoder into memory\n",
    "with open(fc1_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "with open(le_path, 'rb') as l:\n",
    "    le = pickle.load(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data['filename']\n",
    "fc1 = data['features']\n",
    "labels = data['labels']\n",
    "y_gt = le.transform(labels)\n",
    "# print(labels)\n",
    "# print(y_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_gt))\n",
    "print(set(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(fc1[0]))\n",
    "print(fc1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction\n",
    "#### PCA\n",
    "\n",
    "- PCA reduce dimensionality and filter out noise in the data\n",
    "- How many dimensions to keep?\n",
    "- No clear answer, but keep components that contain **signal** (significant variance) and **get rid of ones** that are mostly noise\n",
    "  - A: rule of thumb for high dimensional data is to select 50 components\n",
    "  - inspect how much variance each component preserves before selecting 50\n",
    "  - B: or directly aim for an automated process to determine the amount of components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().set_output()\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "# pca_n = PCA(svd_solver='full')\n",
    "# x_pca_ = pca_n.fit_transform(rescaled) # apply reduction on x\n",
    "\n",
    "# var_ = pca_n.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(4,2),dpi=150,)\n",
    "# ax.grid('on', which='both', color=np.ones(3)*0.85)\n",
    "# ax.plot(range(1,len(var_)+1), var_, color='darkviolet')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_xlabel('number of components')\n",
    "# ax.set_ylabel('fraction of\\nvariance explained')\n",
    "\n",
    "# yt=np.linspace(1/4, 1, 4)\n",
    "# ytm = [np.mean([yt[i], yt[i+1]]) for i in range(len(yt)-1)]\n",
    "# ax.set_yticks(ytm, minor=True)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# #fig.savefig(Path('..','Figures','PCA_Var.png'), bbox_inches='tight')\n",
    "# print('variance preserved by 50 components: {:.3f}'.format(var_[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().set_output()\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "# pca = PCA().fit(rescaled)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance');\n",
    "# print()\n",
    "# var_ = pca.explained_variance_ratio_.cumsum()\n",
    "# print(var_[108])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B Min-Max [0,1] Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select pca data and determine number of components in an automatic fashion\n",
    "# scaler = MinMaxScaler()\n",
    "# data_rescaled = scaler.fit_transform(fc1) # scale data to the range between 0 and 1\n",
    "# pca = PCA(n_components = 0.95) # wand explained variace between 95-99%\n",
    "# pca.fit(data_rescaled)\n",
    "# reduced = pca.fit_transform(data_rescaled)\n",
    "# print(reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1 z Score Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardize values with z-score more effective than min-max and decimal scaling!\n",
    "# # removing mean and scaling to unit variance\n",
    "# # mean of 0 and standard deviation of 1, unlike min-max it does not rescale the feature to a fixed range\n",
    "# rescaled = StandardScaler().fit_transform(fc1) # can be transformed to pandas df\n",
    "# #reduced = rescaled\n",
    "\n",
    "# # normal pca\n",
    "# #scaled_pca = PCA(n_components=0.90, svd_solver='full', random_state=42, whiten=False) # higher values may introduce overfitting of the model, rs only for different svd solver relevant\n",
    "# scaled_pca = PCA(n_components=0.80, svd_solver='full', random_state=42, whiten=False)\n",
    "# scaled_pca.fit(rescaled)\n",
    "# reduced = scaled_pca.fit_transform(rescaled)\n",
    "# print(reduced.shape)\n",
    "\n",
    "# plt.scatter(\n",
    "#     reduced[:, 0],\n",
    "#     reduced[:, 1])\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title('PCA plot', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pca on raw not standardized data\n",
    "pca = PCA(n_components=0.85, svd_solver='full', random_state=42, whiten=True) # higher values may introduce overfitting of the model, rs only for different svd solver relevant, 50 is arbitrary choice\n",
    "pca.fit(fc1)\n",
    "reduced = pca.fit_transform(fc1)\n",
    "print(reduced.shape)\n",
    "\n",
    "plt.scatter(\n",
    "    reduced[:, 0],\n",
    "    reduced[:, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('PCA plot', fontsize=24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap # non linear transformation (compared to pca)\n",
    "# reducer = umap.UMAP(n_components=2, metric='cosine', random_state=666)\n",
    "# scaler = StandardScaler().set_output() # can be transformed to pandas df\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "# reduced = reducer.fit_transform(rescaled)\n",
    "# print(reduced.shape)\n",
    "\n",
    "# plt.scatter(\n",
    "#     reduced[:, 0],\n",
    "#     reduced[:, 1])\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title('UMAP projection Schiefer', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap # non linear transformation (compared to pca)\n",
    "reducer = umap.UMAP(random_state=666)\n",
    "reduced = reducer.fit_transform(fc1)\n",
    "print(reduced.shape)\n",
    "\n",
    "plt.scatter(\n",
    "    reduced[:, 0],\n",
    "    reduced[:, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection Schiefer', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile method\n",
    "# print(\"percent of arr\", np.percentile(reduced[:,0], [25,50,75]))\n",
    "# print(\"percent of arr\", np.percentile(reduced, [25,50,75] , axis = 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# data_rescaled = scaler.fit_transform(fc1) # scale data to the range between 0 and 1\n",
    "# pca = PCA(n_components = 0.95) # wand explained variace between 95-99%\n",
    "# kernel_pca = KernelPCA(\n",
    "#     n_components=None, kernel=\"rbf\", gamma=10, fit_inverse_transform=True, alpha=0.1\n",
    "# )\n",
    "\n",
    "# X_test_kernel_pca = kernel_pca.fit(data_rescaled).fit_transform(data_rescaled)\n",
    "# print(X_test_kernel_pca.shape)\n",
    "# reduced = X_test_kernel_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "\n",
    "# sparse_PCA = SparsePCA(n_components=100, random_state=25)\n",
    "# sparse_PCA.fit(rescaled)\n",
    "# reduced = sparse_PCA.fit_transform(rescaled)\n",
    "# print(reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "# #reduced = rescaled\n",
    "\n",
    "# scaled_pca = PCA(n_components=108, svd_solver='randomized', random_state=27) # higher values may introduce overfitting of the model\n",
    "# scaled_pca.fit(rescaled)\n",
    "# reduced = scaled_pca.fit_transform(rescaled)\n",
    "# print(reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Agglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "# rescaled = scaler.fit_transform(fc1)\n",
    "\n",
    "# agglo = FeatureAgglomeration(n_clusters=5)\n",
    "# agglo.fit(rescaled)\n",
    "# reduced = agglo.transform(rescaled)\n",
    "# print(reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the optimal number of clusters k\n",
    "\n",
    "#### A: Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "kmean_model = KMeans(init='k-means++', n_init='auto', random_state=42)\n",
    "visualizer = KElbowVisualizer(kmean_model, k=(2,15), timings= True)\n",
    "visualizer.fit(reduced)        # Fit data to visualizer\n",
    "visualizer.show()        # Finalize and render figure\n",
    "print(visualizer.elbow_value_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B: Scaled Insertia Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled inertia approach to determine optimal k\n",
    "def kMeansRes(scaled_data, k, alpha_k=0.02):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    scaled_data: matrix \n",
    "        scaled data. rows are samples and columns are features for clustering\n",
    "    k: int\n",
    "        current k for applying KMeans\n",
    "    alpha_k: float\n",
    "        manually tuned factor that gives penalty to the number of clusters\n",
    "    Returns \n",
    "    -------\n",
    "    scaled_inertia: float\n",
    "        scaled inertia value for current k           \n",
    "    '''\n",
    "    \n",
    "    inertia_o = np.square((scaled_data - scaled_data.mean(axis=0))).sum()\n",
    "    # fit k-means\n",
    "    kmeans = KMeans(n_init='auto',init='k-means++', n_clusters=k, random_state=0).fit(scaled_data)\n",
    "    scaled_inertia = kmeans.inertia_ / inertia_o + alpha_k * k\n",
    "    return scaled_inertia\n",
    "\n",
    "def chooseBestKforKMeansParallel(scaled_data, k_range):\n",
    "    '''\n",
    "    Parameters \n",
    "    ----------\n",
    "    scaled_data: matrix \n",
    "        scaled data. rows are samples and columns are features for clustering\n",
    "    k_range: list of integers\n",
    "        k range for applying KMeans\n",
    "    Returns \n",
    "    -------\n",
    "    best_k: int\n",
    "        chosen value of k out of the given k range.\n",
    "        chosen k is k with the minimum scaled inertia value.\n",
    "    results: pandas DataFrame\n",
    "        adjusted inertia value for each k in k_range\n",
    "    '''\n",
    "    \n",
    "    ans = Parallel(n_jobs=-1,verbose=10)(delayed(kMeansRes)(scaled_data, k) for k in k_range)\n",
    "    ans = list(zip(k_range,ans))\n",
    "    results = pd.DataFrame(ans, columns = ['k','Scaled Inertia']).set_index('k')\n",
    "    best_k = results.idxmin()[0]\n",
    "    return best_k, results\n",
    "\n",
    "def chooseBestKforKMeans(scaled_data, k_range):\n",
    "    ans = []\n",
    "    for k in k_range:\n",
    "        scaled_inertia = kMeansRes(scaled_data, k)\n",
    "        ans.append((k, scaled_inertia))\n",
    "    results = pd.DataFrame(ans, columns = ['k','Scaled Inertia']).set_index('k')\n",
    "    best_k = results.idxmin()[0]\n",
    "    return best_k, results\n",
    "\n",
    "# choose k range\n",
    "k_range=range(2,20)\n",
    "# compute adjusted intertia\n",
    "best_k, results = chooseBestKforKMeansParallel(reduced, k_range)\n",
    "print(\"best k is\",best_k)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(results,'o')\n",
    "plt.title('Adjusted Inertia for each K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Adjusted Inertia')\n",
    "plt.xticks(range(2,20,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C: Gap Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimalK(data, nrefs=3, maxClusters=15):\n",
    "#     \"\"\"\n",
    "#     Calculates KMeans optimal K using Gap Statistic from Tibshirani, Walther, Hastie\n",
    "#     Params:\n",
    "#         data: ndarry of shape (n_samples, n_features)\n",
    "#         nrefs: number of sample reference datasets to create\n",
    "#         maxClusters: Maximum number of clusters to test for\n",
    "#     Returns: (gaps, optimalK)\n",
    "#     \"\"\"\n",
    "#     gaps = np.zeros((len(range(1, maxClusters)),))\n",
    "#     resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})\n",
    "#     for gap_index, k in enumerate(range(1, maxClusters)):\n",
    "#         # Holder for reference dispersion results\n",
    "#         refDisps = np.zeros(nrefs)\n",
    "#         # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "#         for i in range(nrefs):\n",
    "            \n",
    "#             # Create new random reference set\n",
    "#             randomReference = np.random.random_sample(size=data.shape)\n",
    "            \n",
    "#             # Fit to it\n",
    "#             km = KMeans(k, n_init='auto')\n",
    "#             km.fit(randomReference)\n",
    "            \n",
    "#             refDisp = km.inertia_\n",
    "#             refDisps[i] = refDisp\n",
    "#         # Fit cluster to original data and create dispersion\n",
    "#         km = KMeans(k, n_init='auto')\n",
    "#         km.fit(data)\n",
    "        \n",
    "#         origDisp = km.inertia_\n",
    "#         # Calculate gap statistic\n",
    "#         gap = np.log(np.mean(refDisps)) - np.log(origDisp)\n",
    "#         # Assign this loop's gap statistic to gaps\n",
    "#         gaps[gap_index] = gap\n",
    "        \n",
    "#         resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)\n",
    "#     return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal\n",
    "\n",
    "# # Automatically output the number of clusters\n",
    "# k, gapdf = optimalK(reduced, nrefs=3, maxClusters=15)\n",
    "# print('Optimal k is: ', k)\n",
    "# # Visualization\n",
    "# plt.plot(gapdf.clusterCount, gapdf.gap, linewidth=3)\n",
    "# plt.scatter(gapdf[gapdf.clusterCount == k].clusterCount, gapdf[gapdf.clusterCount == k].gap, s=250, c='r')\n",
    "# plt.grid(True)\n",
    "# plt.xlabel('Cluster Count')\n",
    "# plt.ylabel('Gap Value')\n",
    "# plt.title('Gap Values by Cluster Count')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D: Silhouette score for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KMeans(init='k-means++', n_init='auto', random_state=42)\n",
    "# # k is range of number of clusters.\n",
    "# visualizer = KElbowVisualizer(model, k=(2,15),metric='silhouette', timings= True)\n",
    "# visualizer.fit(reduced)        # Fit the data to the visualizer\n",
    "# visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E: Calinksi Harabasz Score for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KMeans(init='k-means++', n_init='auto', random_state=42)\n",
    "# # k is range of number of clusters.\n",
    "# visualizer = KElbowVisualizer(model, k=(2,15),metric='calinski_harabasz', timings= True)\n",
    "# visualizer.fit(reduced)        # Fit the data to the visualizer\n",
    "# visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F: Dendogram hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dendogram for Heirarchical Clustering\n",
    "# import scipy.cluster.hierarchy as shc\n",
    "# plt.figure(figsize=(10, 7))  \n",
    "# plt.title(\"Dendrograms\")  \n",
    "# dend = shc.dendrogram(shc.linkage(reduced, method='ward'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-sne/pca visulization on data\n",
    "\n",
    "- distances between well-separated clusters in a t-SNE plot may mean nothing because it justs preserves local structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no whitening\n",
    "# pca_nw = PCA(n_components=0.85, svd_solver='full', whiten=False, random_state=42)\n",
    "# x_nw = pca_nw.fit_transform(fc1)\n",
    "# tsne = TSNE(n_components=2, random_state=567)\n",
    "# x_nw_tsne = tsne.fit_transform(x_nw)\n",
    "# #with withening\n",
    "# pca = PCA(n_components=0.85, svd_solver='full', whiten=True, random_state=42)\n",
    "# x = pca.fit_transform(fc1)\n",
    "# tsne_w = TSNE(n_components=2, random_state=567)\n",
    "# x_w_tsne = tsne_w.fit_transform(x)\n",
    "\n",
    "# labels_ordered = le.inverse_transform(range(len(le.mapper)))\n",
    "# df = pd.DataFrame({'files': files,\n",
    "#                    'x_nw':x_nw_tsne[:,0],\n",
    "#                    'y_nw':x_nw_tsne[:,1],\n",
    "#                    'x_w': x_w_tsne[:,0],\n",
    "#                   'y_w': x_w_tsne[:,1],\n",
    "#                    'labels': labels,\n",
    "#                   },\n",
    "#                   index=files)\n",
    "\n",
    "# fig, ax = plt.subplots(1,2, figsize=(6,3), dpi=150)\n",
    "# sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', palette='tab10', hue_order=labels_ordered,  ax=ax[0])\n",
    "# sns.scatterplot(data=df, x='x_w', y='y_w', hue='labels', palette='tab10', hue_order=labels_ordered, ax=ax[1])\n",
    "# ax[0].get_legend().remove()\n",
    "# ax[1].legend(bbox_to_anchor=(1.05,1))\n",
    "# ax[0].set_title('t-sne/pca\\n without whitening')\n",
    "# ax[1].set_title('t-sne/pca with whitening')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pano plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_nw = PCA(n_components=0.85, svd_solver='full', whiten=False, random_state=42)\n",
    "# x_nw = pca_nw.fit_transform(fc1)\n",
    "# X = np.array(x_nw)\n",
    "# tsne = TSNE(n_components=2, random_state=567).fit_transform(X)\n",
    "\n",
    "# tx, ty = tsne[:,0], tsne[:,1]\n",
    "# tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "# ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_ordered = le.inverse_transform(range(len(le.mapper)))\n",
    "# df = pd.DataFrame({'files': files,\n",
    "#                    'x_nw':tx,\n",
    "#                    'y_nw':ty,\n",
    "#                    'labels': labels,\n",
    "#                   },\n",
    "#                   index=files)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(6,3), dpi=150)\n",
    "# sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', palette='tab10', hue_order=labels_ordered)\n",
    "# ax.get_legend().remove()\n",
    "# ax.legend(bbox_to_anchor=(1.05,1))\n",
    "# ax.set_title('t-sne')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot\n",
    "# from matplotlib.pyplot import imshow\n",
    "\n",
    "# width = 4000\n",
    "# height = 3000\n",
    "# max_dim = 100\n",
    "\n",
    "# full_image = Image.new('RGBA', (width, height))\n",
    "# for img, x, y in zip(files, tx, ty):\n",
    "#     tile = Image.open(img)\n",
    "#     rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "#     tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "#     full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
    "\n",
    "# matplotlib.pyplot.figure(figsize = (16,12))\n",
    "# imshow(full_image)\n",
    "# plt.grid(None)\n",
    "# #plt.savefig('/home/richard/Pictures/temp/tsne_img.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import glob\n",
    "import cv2\n",
    "def visualize_scatter_with_images(X_2d_data, images, figsize=(45,45), image_zoom=1):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    artists = []\n",
    "    for xy, i in zip(X_2d_data, images):\n",
    "        x0, y0 = xy\n",
    "        img = OffsetImage(i, zoom=image_zoom)\n",
    "        ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(X_2d_data)\n",
    "    ax.autoscale()\n",
    "    plt.show()\n",
    "images_path = '/home/richard/data/Schiefer/tests/preprocessed_withpixel_polygon_pred224/'\n",
    "image_files = glob.glob(images_path + '*.png')\n",
    "image_files.sort()\n",
    "images = []\n",
    "for i in image_files:\n",
    "    image = cv2.imread(i)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (100,100))\n",
    "    #image = image.flatten()\n",
    "    images.append(image)\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualize_scatter_with_images(x_nw_tsne, images = [np.reshape(i, (100,100,3)) for i in images], image_zoom=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_ordered = le.inverse_transform(range(len(le.mapper)))\n",
    "# df = pd.DataFrame({'files': files,\n",
    "#                    'x_nw':x_nw_tsne[:,0],\n",
    "#                    'y_nw':x_nw_tsne[:,1],\n",
    "#                    'labels': labels,\n",
    "#                   },\n",
    "#                   index=files)\n",
    "\n",
    "# fig, ax = plt.subplots(1, figsize=(6,3), dpi=150)\n",
    "# sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', hue_order=labels_ordered)\n",
    "# ax.legend(bbox_to_anchor=(1.05,1))\n",
    "# ax.set_title('t-sne/pca\\n without whitening')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = estimate_bandwidth(reduced, quantile=0.3, n_samples=7, random_state=444)\n",
    "\n",
    "model = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "model.fit(reduced)\n",
    "labels_unmatched = model.labels_\n",
    "print(labels_unmatched)\n",
    "y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "\n",
    "cluster_centers = model.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels_unmatched)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_) # cannot be wrong false\n",
    "print(set(labels_unmatched))\n",
    "print(set(y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt clustering\n",
    "# # initialize model\n",
    "model = KMeans(n_clusters=4, init='k-means++', n_init=500, random_state=42)\n",
    "#fit data into the model\n",
    "model.fit(reduced)\n",
    "# extract labels\n",
    "labels_unmatched = model.labels_\n",
    "print(set(labels_unmatched))\n",
    "print(labels_unmatched)\n",
    "y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "print(set(y_pred))\n",
    "print(y_pred)\n",
    "print('inertia: {:.2f}'.format(model.inertia_))\n",
    "print(f\"Silhouette Coefficient: {silhouette_score(reduced, labels_unmatched):.3f}\") # if gt not known\n",
    "print(\"Calinksi-Harabasz index:\", calinski_harabasz_score(reduced, labels_unmatched))\n",
    "print(\"Davies-Bouldin index:\", davies_bouldin_score(reduced, labels_unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "print(len(set(y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying K-medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmedoids = KMedoids(n_clusters=9, random_state=42)\n",
    "# kmedoids.fit(reduced)\n",
    "# labels_unmatched = kmedoids.labels_\n",
    "# print(labels_unmatched)\n",
    "# y_pred = lt.label_matcher(labels_unmatched, y_gt)\n",
    "# print(set(y_pred))\n",
    "# print('inertia: {:.2f}'.format(kmedoids.inertia_))\n",
    "# # application after PCA better accuracy than only rescaled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzy K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCM(n_clusters = 4)\n",
    "model.fit(reduced)\n",
    "\n",
    "centers = model.centers\n",
    "labels_unmatched = model.predict(reduced)\n",
    "print(labels_unmatched)\n",
    "y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "print(y_pred)\n",
    "print(set(labels_unmatched))\n",
    "print(set(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIRCH algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# birch = Birch(threshold=0.5, n_clusters=9).fit(reduced)\n",
    "# labels_unmatched = birch.labels_\n",
    "# y_pred = lt.label_matcher(labels_unmatched, y_gt)\n",
    "# print(labels_unmatched)\n",
    "# print(set(labels_unmatched))\n",
    "# print(set(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering (hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering = AgglomerativeClustering(n_clusters=9).fit(reduced)\n",
    "# labels_unmatched = clustering.labels_\n",
    "# y_pred = lt.label_matcher(labels_unmatched, y_gt)\n",
    "# print(set(labels_unmatched))\n",
    "# print(set(y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # very sensitive to parameters\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(rescaled.shape)\n",
    "# print(reduced.shape)\n",
    "# dbscan = DBSCAN(eps=60, min_samples=150).fit(reduced)\n",
    "# labels_unmatched = dbscan.labels_\n",
    "# labels_true = y_gt\n",
    "# print(labels_unmatched)\n",
    "\n",
    "# # Number of clusters in labels, ignoring noise if present.\n",
    "# n_clusters_ = len(set(labels_unmatched)) - (1 if -1 in labels_unmatched else 0)\n",
    "# n_noise_ = list(labels_unmatched).count(-1)\n",
    "\n",
    "# print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "# print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# print(f\"Homogeneity: {metrics.homogeneity_score(labels_true, labels_unmatched):.3f}\")\n",
    "# print(f\"Completeness: {metrics.completeness_score(labels_true, labels_unmatched):.3f}\")\n",
    "# print(f\"V-measure: {metrics.v_measure_score(labels_true, labels_unmatched):.3f}\")\n",
    "# print(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(labels_true, labels_unmatched):.3f}\")\n",
    "# print(\n",
    "#     \"Adjusted Mutual Information:\"\n",
    "#     f\" {metrics.adjusted_mutual_info_score(labels_true, labels_unmatched):.3f}\"\n",
    "# )\n",
    "# print(f\"Silhouette Coefficient: {metrics.silhouette_score(reduced, labels_unmatched):.3f}\")\n",
    "\n",
    "# y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "\n",
    "# unique_labels = set(labels_unmatched)\n",
    "# core_samples_mask = np.zeros_like(labels_unmatched, dtype=bool)\n",
    "# core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "# colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "# for k, col in zip(unique_labels, colors):\n",
    "#     if k == -1:\n",
    "#         # Black used for noise.\n",
    "#         col = [0, 0, 0, 1]\n",
    "\n",
    "#     class_member_mask = labels == k\n",
    "\n",
    "#     xy = reduced[class_member_mask & core_samples_mask]\n",
    "#     plt.plot(\n",
    "#         xy[:, 0],\n",
    "#         xy[:, 1],\n",
    "#         \"o\",\n",
    "#         markerfacecolor=tuple(col),\n",
    "#         markeredgecolor=\"k\",\n",
    "#         markersize=14,\n",
    "#     )\n",
    "\n",
    "#     xy = reduced[class_member_mask & ~core_samples_mask]\n",
    "#     plt.plot(\n",
    "#         xy[:, 0],\n",
    "#         xy[:, 1],\n",
    "#         \"o\",\n",
    "#         markerfacecolor=tuple(col),\n",
    "#         markeredgecolor=\"k\",\n",
    "#         markersize=6,\n",
    "#     )\n",
    "\n",
    "# plt.title(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True, gen_min_span_tree=False, leaf_size=40, metric='euclidean', min_cluster_size=8, min_samples=None, p=None)\n",
    "# clusterer.fit(reduced)\n",
    "# print(clusterer.labels_)\n",
    "# print(clusterer.labels_.max())\n",
    "\n",
    "# labels_unmatched = clusterer.labels_\n",
    "# y_pred = lt.label_matcher(labels_unmatched, y_gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optics (better suited on large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering = OPTICS(min_samples=5).fit(reduced)\n",
    "# labels_unmatched = clustering.labels_\n",
    "# print(labels_unmatched)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering = SpectralClustering(n_clusters=5, assign_labels='discretize', random_state=42).fit(reduced)\n",
    "# labels_unmatched = clustering.labels_\n",
    "# y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "# print(labels_unmatched)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap = AffinityPropagation(preference=-11000, random_state=14).fit(rescaled)\n",
    "# # cluster_centers_indices = ap.cluster_centers_indices_\n",
    "# # labels_unmatched = ap.labels_\n",
    "# # n_clusters_ = len(cluster_centers_indices)\n",
    "# # print(n_clusters_)\n",
    "\n",
    "# # y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "# # print(labels_unmatched)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSNE predictions and ground truth plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#rescaled = scaler.fit_transform(fc1)\n",
    "# no whitening\n",
    "pca_nw = PCA(n_components=0.85, svd_solver='full', whiten=False, random_state=42)\n",
    "x_nw = pca_nw.fit_transform(fc1)\n",
    "tsne = TSNE(n_components=2, random_state=567)\n",
    "x_nw_tsne = tsne.fit_transform(x_nw)\n",
    "#with withening\n",
    "pca = PCA(n_components=0.85, svd_solver='full', whiten=True, random_state=42)\n",
    "x = pca.fit_transform(fc1)\n",
    "tsne_w = TSNE(n_components=2, random_state=567)\n",
    "x_w_tsne = tsne_w.fit_transform(x)\n",
    "\n",
    "labels_ordered = le.inverse_transform(range(len(le.mapper)))\n",
    "df = pd.DataFrame({'files': files,\n",
    "                   'x_nw':x_nw_tsne[:,0],\n",
    "                   'y_nw':x_nw_tsne[:,1],\n",
    "                   'x_w': x_w_tsne[:,0],\n",
    "                  'y_w': x_w_tsne[:,1],\n",
    "                   'labels': labels,\n",
    "                  },\n",
    "                  index=files)\n",
    "\n",
    "y_pred_str = le.inverse_transform(y_pred)\n",
    "y_gt_str = le.inverse_transform(y_gt)\n",
    "df['y_pred_labels'] = pd.Series(y_pred_str, index=files)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,5), dpi=150)\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='labels', palette='tab10', hue_order=labels_ordered, ax=ax[0]) # ground truth labels\n",
    "sns.scatterplot(data=df, x='x_nw', y='y_nw', hue='y_pred_labels', palette='tab10', hue_order=labels_ordered, ax=ax[1]) # predicted labels\n",
    "\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].legend(bbox_to_anchor=(1.05,1))\n",
    "ax[0].set_title('ground truth labels')\n",
    "ax[1].set_title('predicted labels')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Performance (with known truth labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_cm(cm, labelnames, cscale=0.6, ax0=None, fs=5, cmap='cool'):\n",
    "    \"\"\"\n",
    "    Generates a pretty-formated confusion matrix for convenient visualization.\n",
    "    \n",
    "    The true labels are displayed on the rows, and the predicted labels are displayed on the columns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cm: ndarray \n",
    "        nxn array containing the data of the confusion matrix.\n",
    "    \n",
    "    labelnames: list(string)\n",
    "        list of class names in order on which they appear in the confusion matrix. For example, the first\n",
    "        element should contain the class corresponding to the first row and column of *cm*.\n",
    "    cscale: float\n",
    "        parameter that adjusts the color intensity. Allows color to be present for confusion matrices with few mistakes,\n",
    "        and controlling the intensity for ones with many misclassifications.\n",
    "    \n",
    "    ax0: None or matplotlib axis object\n",
    "        if None, a new figure and axis will be created and the visualization will be displayed.\n",
    "        if an axis is supplied, the confusion matrix will be plotted on the axis in place.\n",
    "    fs: int\n",
    "        font size for text on confusion matrix.\n",
    "        \n",
    "    cmap: str\n",
    "        matplotlib colormap to use\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    acc = cm.trace() / cm.sum()\n",
    "    if ax0 is None:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(2.5, 2.5), dpi=300)\n",
    "        fig.set_facecolor('w')\n",
    "    else:\n",
    "        ax = ax0\n",
    "\n",
    "    n = len(labelnames)\n",
    "    ax.imshow(np.power(cm, cscale), cmap=cmap, extent=(0, n, 0, n))\n",
    "    labelticks = np.arange(n) + 0.5\n",
    "    \n",
    "    ax.set_xticks(labelticks, minor=True)\n",
    "    ax.set_yticks(labelticks, minor=True)\n",
    "    ax.set_xticklabels(['' for i in range(n)], minor=False, fontsize=fs)\n",
    "    ax.set_yticklabels(['' for i in range(n)], minor=False, fontsize=fs)\n",
    "    \n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_yticks(np.arange(n))\n",
    "    ax.set_xticklabels(labels=labelnames, minor=True, fontsize=fs, rotation=90)\n",
    "    ax.set_yticklabels(labels=reversed(labelnames), minor=True, fontsize=fs)\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels', fontsize=fs)\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    \n",
    "    ax.set_ylabel('Actual Labels', fontsize=fs)\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        ax.text(j + 0.5, n - i - 0.5, '{:^5}'.format(z), ha='center', va='center', fontsize=fs,\n",
    "                bbox=dict(boxstyle='round', facecolor='w', edgecolor='0.3'))\n",
    "    ax.grid(which='major', color=np.ones(3) * 0.33, linewidth=1)\n",
    "\n",
    "    if ax0 is None:\n",
    "        ax.set_title('Accuracy: {:.3f}'.format(cm.trace() / cm.sum()), fontsize=fs+2)\n",
    "        plt.show()\n",
    "        return\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "# The confusion matrix allows us to evaluate the performance\n",
    "labels_ordered = le.inverse_transform(range(len(le.mapper)))\n",
    "CM = confusion_matrix(y_gt, y_pred)\n",
    "pretty_cm(CM, labels_ordered)\n",
    "print('Accuracy: {:.3f}'.format(CM.trace()/CM.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # or just lucky?\n",
    "# # This takes a couple minutes to run, be patient!\n",
    "# use_cache=True\n",
    "# std_10run_cache_path = Path('strunresults.pickle')\n",
    "\n",
    "# if not use_cache or not std_10run_cache_path.is_file():\n",
    "#     rs = np.random.RandomState(seed=987654321)\n",
    "#     accuracies = np.zeros(10)\n",
    "#     for i, seed in enumerate(rs.randint(2**32, size=10)):\n",
    "#         kmeans_ = model = KMeans(n_clusters=4, init='k-means++', n_init=500, random_state=seed)\n",
    "#         kmeans_.fit(reduced)\n",
    "#         labels_unmatched_ = kmeans_.labels_\n",
    "#         y_pred_ = label_matcher(labels_unmatched_, y_gt)\n",
    "#         acc = (y_pred_ == y_gt).sum()/len(y_gt)\n",
    "#         accuracies[i] = acc\n",
    "    \n",
    "#     with open(std_10run_cache_path, 'wb') as f:\n",
    "#         pickle.dump({'accuracies':accuracies}, f)\n",
    "# else:\n",
    "#     with open(std_10run_cache_path, 'rb') as f:\n",
    "#         results_ = pickle.load(f)\n",
    "#         accuracies = results_['accuracies']\n",
    "\n",
    "# print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(y_true, y_pred):\n",
    "    return np.sum(np.multiply([i==True for i in y_pred], y_true))\n",
    "\n",
    "def fp(y_true, y_pred):\n",
    "    return np.sum(np.multiply([i==True for i in y_pred], [not(j) for j in y_true]))\n",
    "\n",
    "def tn(y_true, y_pred):\n",
    "    return np.sum(np.multiply([i==False for i in y_pred], [not(j) for j in y_true]))\n",
    "\n",
    "def fn(y_true, y_pred):\n",
    "    return np.sum(np.multiply([i==False for i in y_pred], y_true))\n",
    "\n",
    "def get_multiclass_cm_values(y_true, y_pred):\n",
    "    tp_values = []\n",
    "    fp_values = []\n",
    "    tn_values = []\n",
    "    fn_values = []\n",
    "    for i in np.unique(y_true):\n",
    "        modified_true = [i==j for j in y_true]\n",
    "        modified_pred = [i==j for j in y_pred]\n",
    "        TP = tp(modified_true, modified_pred)\n",
    "        tp_values.append(TP)\n",
    "        FP = fp(modified_true, modified_pred)\n",
    "        fp_values.append(FP)\n",
    "        TN = tn(modified_true, modified_pred)\n",
    "        tn_values.append(TN)\n",
    "        FN = fn(modified_true, modified_pred)\n",
    "        fn_values.append(FN)\n",
    "    return np.mean(tp_values), np.mean(fp_values), np.mean(fp_values),  np.mean(fp_values)\n",
    "\n",
    "def f_star(y_true, y_pred):\n",
    "    TP, FP, TN, FN = get_multiclass_cm_values(y_true, y_pred)\n",
    "    return TP / (FN + FP + TP)\n",
    "\n",
    "def go_tp(y_true, y_pred): # for one class\n",
    "    tp_values = []\n",
    "    for i in np.unique(y_true):\n",
    "        modified_true = [i==j for j in y_true]\n",
    "        modified_pred = [i==j for j in y_pred]\n",
    "        score = tp(modified_true, modified_pred)\n",
    "        tp_values.append(score)\n",
    "    print(tp_values)\n",
    "    return np.mean(tp_values)\n",
    "\n",
    "def go_fp(y_true, y_pred): # for one class\n",
    "    tp_values = []\n",
    "    for i in np.unique(y_true):\n",
    "        modified_true = [i==j for j in y_true]\n",
    "        modified_pred = [i==j for j in y_pred]\n",
    "        score = fp(modified_true, modified_pred)\n",
    "        tp_values.append(score)\n",
    "    print(tp_values)\n",
    "    return np.mean(tp_values)\n",
    "\n",
    "def go_tn(y_true, y_pred): # for one class\n",
    "    tp_values = []\n",
    "    for i in np.unique(y_true):\n",
    "        modified_true = [i==j for j in y_true]\n",
    "        modified_pred = [i==j for j in y_pred]\n",
    "        score = tn(modified_true, modified_pred)\n",
    "        tp_values.append(score)\n",
    "    print(tp_values)\n",
    "    return np.mean(tp_values)\n",
    "\n",
    "def go_fn(y_true, y_pred):\n",
    "    tp_values = []\n",
    "    for i in np.unique(y_true):\n",
    "        modified_true = [i==j for j in y_true]\n",
    "        modified_pred = [i==j for j in y_pred]\n",
    "        score = fn(modified_true, modified_pred)\n",
    "        tp_values.append(score)\n",
    "    print(tp_values)\n",
    "    return np.mean(tp_values)\n",
    "\n",
    "true_p = go_tp(y_gt, y_pred)\n",
    "false_p = go_fp(y_gt, y_pred)\n",
    "true_n = go_tn(y_gt, y_pred)\n",
    "false_n = go_fn(y_gt, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual micro f1\n",
    "true_p / (true_p + false_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f star formula\n",
    "res = true_p / (false_n + false_p + true_p)\n",
    "print(res)\n",
    "\n",
    "print(tp(y_gt, y_pred))\n",
    "print(f_star(y_gt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred): # for one class\n",
    "  TP = np.sum(np.multiply([i==True for i in y_pred], y_true))\n",
    "  TN = np.sum(np.multiply([i==False for i in y_pred], [not(j) for j in y_true]))\n",
    "  FP = np.sum(np.multiply([i==True for i in y_pred], [not(j) for j in y_true]))\n",
    "  FN = np.sum(np.multiply([i==False for i in y_pred], y_true))\n",
    "  precision = TP/(TP+FP)\n",
    "  recall = TP/(TP+FN)\n",
    "  if precision != 0 and recall != 0:\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "  else:\n",
    "    f1 = 0\n",
    "  return f1\n",
    "\n",
    "def f1_macro(y_true, y_pred):\n",
    "  macro = []\n",
    "  for i in np.unique(y_true):\n",
    "    modified_true = [i==j for j in y_true]\n",
    "    modified_pred = [i==j for j in y_pred]\n",
    "    score = f1(modified_true, modified_pred)\n",
    "    print(score)\n",
    "    macro.append(score)\n",
    "  return np.mean(macro)\n",
    "\n",
    "print(\"\\n\",f1_macro(y_gt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(actual, predicted, label):\n",
    "\n",
    "    \"\"\" A helper function to calculate f1-score for the given `label` \"\"\"\n",
    "\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    tp = np.sum((actual==label) & (predicted==label))\n",
    "    fp = np.sum((actual!=label) & (predicted==label))\n",
    "    fn = np.sum((predicted!=label) & (actual==label))\n",
    "    \n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    if precision != 0 and recall != 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def f1_macro(actual, predicted):\n",
    "    # `macro` f1- unweighted mean of f1 per label\n",
    "    return np.mean([f1(actual, predicted, label) \n",
    "        for label in np.unique(actual)])\n",
    "\n",
    "print(f1_macro(y_gt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error, rand_score, adjusted_rand_score, mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score\n",
    "print(\"F1-score\")\n",
    "print(f1_score(y_gt, y_pred, average='macro')) # bigger penalty when model performs not well on minority classes\n",
    "print(f1_score(y_gt, y_pred, average='micro')) # no favoring any class in particular\n",
    "print(f1_score(y_gt, y_pred, average='weighted')) # favoring the majority class\n",
    "# mean squared error\n",
    "print(\"\\nMSE\")\n",
    "print(mean_squared_error(y_gt, y_pred))\n",
    "print(mean_squared_error(y_gt, y_pred, squared=False))\n",
    "print(\"\\nMAE\")\n",
    "print(mean_absolute_error(y_gt, y_pred))\n",
    "# rand score\n",
    "print(\"\\nRand index\")\n",
    "print(rand_score(y_gt, y_pred))\n",
    "print(adjusted_rand_score(y_gt, y_pred))\n",
    "# mututal information based scores\n",
    "print(\"\\nMutual information based scores\")\n",
    "print(mutual_info_score(y_gt, y_pred))\n",
    "print(adjusted_mutual_info_score(y_gt, y_pred))\n",
    "print(normalized_mutual_info_score(y_gt, y_pred))\n",
    "# Homogenity, completeness and v-measure\n",
    "print(\"\\nHomogeneity, completeness and v-measure\")\n",
    "print(homogeneity_score(y_gt, y_pred))\n",
    "print(completeness_score(y_gt, y_pred))\n",
    "print(v_measure_score(y_gt, y_pred))\n",
    "# fowlkes\n",
    "print(\"\\nFowlkes-mallows-score\")\n",
    "print(fowlkes_mallows_score(y_gt, y_pred))\n",
    "# Cohen-kappa\n",
    "print(\"\\nCohen-kappa\")\n",
    "print(cohen_kappa_score(y_gt, y_pred))\n",
    "# matthew's correlation coefficient\n",
    "print(\"\\nMCC\")\n",
    "print(matthews_corrcoef(y_gt, y_pred))\n",
    "# log loss\n",
    "# print(\"\\nLog Loss\")\n",
    "# pred = model.predict(reduced)\n",
    "# print(log_loss(y_gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all labels\n",
    "\n",
    "# # normal pca\n",
    "# scaled_pca = PCA(n_components=2) # higher values may introduce overfitting of the model\n",
    "# scaled_pca.fit(rescaled)\n",
    "# reduced = scaled_pca.fit_transform(rescaled)\n",
    "# print(reduced.shape)\n",
    "\n",
    "labels_plot = y_pred_str\n",
    "print(set(labels_plot))\n",
    "u_labels = np.unique(labels_plot)\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(reduced[labels_plot == i , 0] , reduced[labels_plot == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot all labels\n",
    "labels_plot_gt = labels_unmatched\n",
    "labels_plot_gt = le.inverse_transform(labels_plot_gt)\n",
    "print(set(labels_plot_gt))\n",
    "u_labels = np.unique(labels_plot_gt)\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(reduced[labels_plot_gt == i , 0] , reduced[labels_plot_gt == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df, optional\n",
    "df = pd.DataFrame(columns=['filename','label'])\n",
    "df['filename'] = files\n",
    "df['label'] = y_pred_str\n",
    "print(len(df))\n",
    "\n",
    "df.sort_values(by=['filename'],inplace=True)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df.head())\n",
    "#df.to_csv('/home/richard/data/Schiefer/clustering_results_pred.csv')\n",
    "# TODO: Include values from clustering alg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 8\n",
    "n_col = 2\n",
    "_, axs = plt.subplots(n_row, n_col, figsize=(20, 20))\n",
    "axs = axs.flatten()\n",
    "imgs = df['filename']\n",
    "for img_path, ax in zip(imgs[0:n_row*n_col], axs):\n",
    "    img = Image.open(img_path)\n",
    "    label_value = df.loc[df.filename == img_path, 'label'].values[0]\n",
    "    ax.set_title(label_value)\n",
    "    ax.imshow(img)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_number = 0\n",
    "filtered_labels0 = reduced[labels_plot == c_number]\n",
    "print(labels_ordered[c_number])\n",
    "plt.title(labels_ordered[c_number])\n",
    "plt.scatter(filtered_labels0[:,0] , filtered_labels0[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows of original data\n",
    "filtered_label2 = reduced[labels_plot == 0]\n",
    " \n",
    "filtered_label8 = reduced[labels_plot == 3]\n",
    " \n",
    "#Plotting the results\n",
    "plt.title(labels_ordered[0] + \"--> red |_| \" +  labels_ordered[3] + \"--> black\")\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label8[:,0] , filtered_label8[:,1] , color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all labels predictions\n",
    "u_labels = np.unique(labels_plot)\n",
    "print(u_labels)\n",
    "\n",
    "for i in u_labels:\n",
    "    plt.scatter(reduced[labels_plot == i , 0] , reduced[labels_plot == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### T-SNE Visulization\n",
    "\n",
    "T-SNE maps components in high-dimensional space to lower dimensions. Used to project data to 2d for visulization. Technique preserve pairwise distances for points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# data_rescaled = scaler.fit_transform(fc1) # scale data to the range between 0 and 1\n",
    "\n",
    "# # tsne\n",
    "# pca_nw = PCA(n_components=0.95, svd_solver='full', whiten=False)\n",
    "# x_nw = pca_nw.fit_transform(data_rescaled)\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=12214) # reducing dimension to two\n",
    "# x_nw_tsne = tsne.fit_transform(x_nw)\n",
    "\n",
    "# # scale coordinates to fit 0:1\n",
    "# def scale_to_01_range(x):\n",
    "#     # compute the distribution range\n",
    "#     value_range = (np.max(x) - np.min(x))\n",
    " \n",
    "#     # move the distribution so that it starts from zero\n",
    "#     # by extracting the minimal value from all its values\n",
    "#     starts_from_zero = x - np.min(x)\n",
    " \n",
    "#     # make the distribution fit [0; 1] by dividing by its range\n",
    "#     return starts_from_zero / value_range\n",
    " \n",
    "# # extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "# tx = x_nw_tsne[:, 0]\n",
    "# ty = x_nw_tsne[:, 1]\n",
    " \n",
    "# tx = scale_to_01_range(tx)\n",
    "# ty = scale_to_01_range(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'files': files,\n",
    "#                    'labels': labels,\n",
    "#                   },\n",
    "#                   index=files)\n",
    "\n",
    "# fig, ax = plt.subplots(1, figsize=(6,3), dpi=150)\n",
    "# sns.scatterplot(data=df, x=tx, y=ty, hue='labels')\n",
    "# ax.legend(bbox_to_anchor=(1.05,1))\n",
    "# ax.set_title('t-sne/pca\\n without whitening')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spectral clustering --> bad results\n",
    "# import sklearn.datasets as skl_data\n",
    "# import sklearn.cluster as skl_cluster\n",
    "\n",
    "# graph_model = skl_cluster.SpectralClustering(n_clusters=4, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# gr = scaler.fit_transform(fc1) # scale data to the range between 0 and 1\n",
    "# pca = PCA(n_components = 0.95) # wand explained variace between 95-99%\n",
    "# pca.fit(gr)\n",
    "# graph = pca.fit_transform(gr)\n",
    "\n",
    "# graph_labels = model.fit_predict(graph)\n",
    "\n",
    "# plt.scatter(fc1[:, 0], fc1[:, 1], s=15, linewidth=0, c=labels, cmap='flag')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# Y = scaler.fit_transform(fc1) # scale data to the range between 0 and 1\n",
    "# pca = PCA(n_components = 0.95) # wand explained variace between 95-99%\n",
    "# pca.fit(Y)\n",
    "# X = pca.fit_transform(Y)\n",
    "\n",
    "# range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "# for n_clusters in range_n_clusters:\n",
    "#     # Create a subplot with 1 row and 2 columns\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     fig.set_size_inches(18, 7)\n",
    "\n",
    "#     # The 1st subplot is the silhouette plot\n",
    "#     # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "#     # lie within [-0.1, 1]\n",
    "#     ax1.set_xlim([-0.1, 1])\n",
    "#     # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "#     # plots of individual clusters, to demarcate them clearly.\n",
    "#     ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "#     # Initialize the clusterer with n_clusters value and a random generator\n",
    "#     # seed of 10 for reproducibility.\n",
    "#     clusterer = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n",
    "#     cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "#     # The silhouette_score gives the average value for all the samples.\n",
    "#     # This gives a perspective into the density and separation of the formed\n",
    "#     # clusters\n",
    "#     silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#     print(\n",
    "#         \"For n_clusters =\",\n",
    "#         n_clusters,\n",
    "#         \"The average silhouette_score is :\",\n",
    "#         silhouette_avg,\n",
    "#     )\n",
    "\n",
    "#     # Compute the silhouette scores for each sample\n",
    "#     sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "#     y_lower = 10\n",
    "#     for i in range(n_clusters):\n",
    "#         # Aggregate the silhouette scores for samples belonging to\n",
    "#         # cluster i, and sort them\n",
    "#         ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "#         ith_cluster_silhouette_values.sort()\n",
    "\n",
    "#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#         y_upper = y_lower + size_cluster_i\n",
    "\n",
    "#         color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "#         ax1.fill_betweenx(\n",
    "#             np.arange(y_lower, y_upper),\n",
    "#             0,\n",
    "#             ith_cluster_silhouette_values,\n",
    "#             facecolor=color,\n",
    "#             edgecolor=color,\n",
    "#             alpha=0.7,\n",
    "#         )\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#     ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "#     ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "#     ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "#     # The vertical line for average silhouette score of all the values\n",
    "#     ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "#     ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#     ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "#     # 2nd Plot showing the actual clusters formed\n",
    "#     colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "#     ax2.scatter(\n",
    "#         X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "#     )\n",
    "\n",
    "#     # Labeling the clusters\n",
    "#     centers = clusterer.cluster_centers_\n",
    "#     # Draw white circles at cluster centers\n",
    "#     ax2.scatter(\n",
    "#         centers[:, 0],\n",
    "#         centers[:, 1],\n",
    "#         marker=\"o\",\n",
    "#         c=\"white\",\n",
    "#         alpha=1,\n",
    "#         s=200,\n",
    "#         edgecolor=\"k\",\n",
    "#     )\n",
    "\n",
    "#     for i, c in enumerate(centers):\n",
    "#         ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "#     ax2.set_title(\"The visualization of the clustered data.\")\n",
    "#     ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "#     ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "#     plt.suptitle(\n",
    "#         \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "#         % n_clusters,\n",
    "#         fontsize=14,\n",
    "#         fontweight=\"bold\",\n",
    "#     )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualizations and normality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(fc1[:,1],fc1[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(rescaled[:,1],rescaled[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(reduced[:,1],reduced[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = reduced[1]\n",
    "# #dataset = fc1\n",
    "\n",
    "# # data plots after pca\n",
    "# print(dataset.shape)\n",
    "# # histogram\n",
    "# plt.hist(dataset)\n",
    "# plt.title(\"Histogram\")\n",
    "# plt.show()\n",
    "# # qq-plot\n",
    "# qq_plot = sm.qqplot(dataset, line='45')\n",
    "# plt.title(\"qq-plot\")\n",
    "# plt.show()\n",
    "# # shapiro,wilk test\n",
    "# stat, p = shapiro(dataset)\n",
    "# print(\"statistics=%.3f, p=%.3f\" % (stat,p))\n",
    "# alpha = 0.05\n",
    "# if p > alpha:\n",
    "#     print(\"sample looks like gaussian\")\n",
    "# else:\n",
    "#     print(\"sample does not look gaussian\")\n",
    "# # kstest kolmogorov smirnov test\n",
    "# print(kstest(dataset, 'norm'))\n",
    "# # D'Agostinos K2 Test\n",
    "# stat, p = normaltest(dataset)\n",
    "# print(\"statistics=%.3f, p=%.3f\" % (stat,p))\n",
    "# alpha = 0.05\n",
    "# if p > alpha:\n",
    "#     print(\"sample looks like gaussian\")\n",
    "# else:\n",
    "#     print(\"sample does not look gaussian\")\n",
    "# # anderson darling test\n",
    "# result = anderson(dataset)\n",
    "# print('Statistic: %.3f' % result.statistic)\n",
    "# p = 0\n",
    "# for i in range(len(result.critical_values)):\n",
    "#     sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "#     if result.statistic < result.critical_values[i]:\n",
    "#         print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "#     else:\n",
    "#         print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "NUMBER_OF_CLASSES = 4\n",
    "def agglo_cl(reduced_f, y_gt):\n",
    "    model = AgglomerativeClustering(n_clusters=NUMBER_OF_CLASSES)\n",
    "    model.fit(reduced_f)\n",
    "    labels_unmatched = model.labels_\n",
    "    y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "    num_pred_species = len(set(y_pred))\n",
    "    return y_pred, num_pred_species\n",
    "\n",
    "def fuzzy_k_means(reduced_f, y_gt):\n",
    "    model = FCM(n_clusters=NUMBER_OF_CLASSES)\n",
    "    model.fit(reduced_f)\n",
    "    labels_unmatched = model.predict(reduced_f)\n",
    "    y_pred = label_matcher(labels_unmatched, y_gt)\n",
    "    num_pred_species = len(set(y_pred))\n",
    "    return y_pred, num_pred_species\n",
    "\n",
    "def get_accuracy_value(y_gt, y_pred):\n",
    "    micro_f1_score = f1_score(y_gt, y_pred, average='micro')\n",
    "    macro_f1_score = f1_score(y_gt, y_pred, average='macro')\n",
    "    nmi = v_measure_score(y_gt, y_pred) # cluster evaluation\n",
    "    return micro_f1_score, macro_f1_score, nmi\n",
    "\n",
    "fns = [agglo_cl, fuzzy_k_means]\n",
    "fns_strings = str(fns)\n",
    "print(fns_strings)\n",
    "\n",
    "for fn in fns:\n",
    "    res, b = fn(reduced, y_gt)\n",
    "    \n",
    "    print(res, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectree2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "579d37c0e11f829fd27710afca693474f5bbc510c142a7662cee2b0a86b87b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
